{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Steps***  \n",
    "***Part 1: Log Collection/Cleaning***\n",
    "1. Run `parser.py` file in '/Parsing'. Will parse files in 'rawlogs' into 'cleanlogs'. Creates log_structured.csv and log_templates.csv.\n",
    "- Example code: `%run parser.py`\n",
    "2. Use `clean_logs()` to clean log_structured.csv. Rename events to E1, E2 etc., creates log_structured_clean.csv.\n",
    "- Example code: `clean_logs(\"./Parsing/cleanlogs/Zookeeper_2k.log_structured.csv\",\"./Parsing/cleanlogs/Zookeeper_2k.log_templates.csv\",\"Zookeeper\")`.  \n",
    "2. 1:  For labeled data use `label_log()` to label your log_structured_clean.csv log into log_labelled.csv. ***Currently for HDFS only***\n",
    "- Example code: `label_log(\"./Parsing/cleanlogs/HDFS_structured_clean.csv\",\"./Parsing/rawlogs/anomaly_label.csv\")`.  \n",
    "     \n",
    "***Part 2: Log Parsing and Feature Extraction***  \n",
    "3. Use `_session_window()` or `_fixed_window()` to parse and extract events from your log_structured_clean.csv or log_labelled.csv file. ***`_session_window()` for HDFS only***\n",
    "- Example code (labeled): `x,y,df = _session_window(data_file=\"./Parsing/cleanlogs/HDFS_labelled.csv\",labels=True)`.  \n",
    "- Example code: `x,y,df = _fixed_window(data_file=\"./Parsing/cleanlogs/Zookeeper_structured_clean.csv\",windowsize=2,log_type=\"Zookeeper\")`.  \n",
    "4. Use `_split_data()` to split x,y into training and testing sets.  ***split_type = 'uniform' is only for Labeled Data***.  \n",
    "- Example code (labeled): = `(x_train,y_train),(x_test,y_test) = _split_data(x,y,train_ratio=0.5,split_type=\"uniform\")`.  \n",
    "- Example code (unlabeled): `(x_train, _),( x_test, _) = _split_data(x,y=None,train_ratio=0.5,split_type=\"sequential\")`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Reading in Data\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "#For Creating labelled csv\n",
    "from ast import literal_eval #For keeping ParameterList as List when reading in CSV \n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "#For Plot of Anomalies\n",
    "import altair as alt\n",
    "\n",
    "#For Log Parsing/Feature Extraction\n",
    "import regex as re\n",
    "from sklearn.utils import shuffle\n",
    "from collections import OrderedDict\n",
    "from datetime import timedelta   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### CSV Cleaning for Logs\n",
    "###\n",
    "def clean_logs(data_file,template_file,log_type):\n",
    "    ###\n",
    "    ###Clean log_structued.csv into new log_strucuted_clean.csv.\n",
    "    ###\n",
    "    ###Parameters: data_file, str (file path to log_strucutred.csv)\n",
    "    ###Parameters: template_file, str (file path to log_template.csv)\n",
    "    ###Parameters: log_type, str (for output file naming)\n",
    "    ###\n",
    "    ###Output: csv file\n",
    "    \n",
    "    #Read in the files from parser.py\n",
    "    df_raw = pd.read_csv(data_file)\n",
    "    template = pd.read_csv(template_file)\n",
    "    \n",
    "    \n",
    "    #Create more clear EventIds: E1, E2\n",
    "    template[\"Clean_EventId\"] = [\"E\"+str(i+1) for i in range(0,len(template.EventId))]\n",
    "    #Store new EventIds into Dictionary\n",
    "    events = dict(zip(template.EventId,template.Clean_EventId))\n",
    "    #Rename EventIds to E1,E2 ...\n",
    "    df_raw.EventId = df_raw.EventId.map(events)\n",
    "\n",
    "    #Create new CSV file log_structured_clean.csv\n",
    "    df_raw.to_csv(\"./Parsing/cleanlogs/\"+log_type+\"_structured_clean.csv\",sep=\",\",index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logs(\"./Parsing/cleanlogs/HDFS_2k.log_structured.csv\",\"./Parsing/cleanlogs/HDFS_2k.log_templates.csv\",\"HDFS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_log(data_file,label_file,log_type=\"HDFS\"):\n",
    "    ###\n",
    "    ###Currently tailored to HDFS (looking at blkid as unique identifiers)\n",
    "    ###Creates a log_labnelled.csv file\n",
    "    ###\n",
    "    ###Parameters: data_file, str (file path to log_structued_clean.csv)\n",
    "    ###Parameters: label_file, str (file path to label file)\n",
    "    ###Parameters: log_type, str (for output file naming)\n",
    "    ###\n",
    "    ###Output: csv file\n",
    "    \n",
    "    #Reading in Data\n",
    "    anomaly = pd.read_csv(label_file)\n",
    "    anomaly_labels = anomaly.set_index(\"BlockId\")[\"Label\"].to_dict()\n",
    "    log_clean = pd.read_csv(data_file,converters={'ParameterList': literal_eval})\n",
    "    warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning) #Ignore some warning messages \n",
    "    \n",
    "    #Some functions for extracting block id\n",
    "    def extract_blk_id(x):\n",
    "    ###\n",
    "    ###Check if entry is a blk_id, return True if value is a blk_id \n",
    "    ###\n",
    "    ###Input: x, str\n",
    "    ###Output: boolean\n",
    "        if \"blk\" in x:\n",
    "            return True\n",
    "        \n",
    "    #Individual Entries inside blk_id\n",
    "    def extract_label(x): \n",
    "        ###\n",
    "        ###Determine if given list of bulk_ids are normal, anomaly or not specified, return str. \n",
    "        ###In the case of multiple bulk_ids, if 1 blk_id corresponds to an Anomaly - will be classified as Anomaly. \n",
    "        ###If the blk_id is not in our dictionary of labels - will be classified as Unavailable.\n",
    "        ###\n",
    "        ###Input: x, list of blk_ids\n",
    "        ###Output: label, str - either Normal, Anomaly or Unavailable\n",
    "        for blk_id in x: #Go through list of blk_ids\n",
    "            if blk_id in anomaly_labels.keys(): #Check if the id is in out list of labels\n",
    "                \n",
    "                if anomaly_labels[blk_id] == \"Anomaly\": #Find any Anomalies\n",
    "                    return \"Anomaly\" \n",
    "                else:\n",
    "                    return \"Normal\"\n",
    "                \n",
    "            else: #id does not exist in our dictionary (i.e. do not know label)\n",
    "                return \"Unavailable\"\n",
    "\n",
    "    #Create blk_id column\n",
    "    count = 0\n",
    "    log_clean[\"blk_id\"] = \"\" #Initialize new column\n",
    "    for params in log_clean.ParameterList:\n",
    "        if params == []: #If the list is empty - when the IPLOM parser failed (on 3 occaisions) @ line 1438,1767,1900\n",
    "            #Need to go to Content to get blk_id\n",
    "            intermediate = [blkid for blkid in log_clean[\"Content\"][count].split() if extract_blk_id(blkid)] \n",
    "            if len(intermediate) == 1:\n",
    "                log_clean[\"blk_id\"][count] = intermediate\n",
    "            else: #BLOCK* ask 10.251.126.5 50010 to delete: Only want the blk_id from * not what is deleted\n",
    "                 log_clean[\"blk_id\"][count] = [intermediate[0]]\n",
    "        else: \n",
    "            log_clean[\"blk_id\"][count] = [blkid for blkid in params if extract_blk_id(blkid)] #Extract parameter entries with blk_id\n",
    "        count+=1\n",
    "\n",
    "    #Deal with weird cases such as: \n",
    "    #['blk_-7601381921195183756', 'mnt/hadoop/dfs/data/current/subdir48/blk_-7601381921195183756'] = Keep 1 blk_id\n",
    "    #or \n",
    "    #[]'/user/root/randtxt2/_temporary/_task_200811101024_0002_m_000299_0/part-00299. blk_4984150784048864430']  or ['replicate blk_-7571492020523929240 to datanode(s) 10.251.122.38']= Extract blk_id\n",
    "    count = 0\n",
    "    for row in log_clean.blk_id: \n",
    "        if any(\"/\" in sub for sub in row) or any(\" blk\" in sub for sub in row): #Rows with weird cases similar to above\n",
    "            if len(row) == 2:\n",
    "                temp = row[0]\n",
    "                log_clean[\"blk_id\"][count] = [temp]\n",
    "            else:\n",
    "                temp = row[0].split(\" \")\n",
    "                idx = [i for i, s in enumerate(temp) if 'blk' in s][0]\n",
    "                log_clean[\"blk_id\"][count] = [temp[idx]]\n",
    "        count += 1\n",
    "        \n",
    "    #Create label column\n",
    "    log_clean[\"label\"] = [extract_label(blkid) for blkid in log_clean.blk_id] #Initialize new column\n",
    "\n",
    "    #Export to CSV\n",
    "    log_clean.to_csv(\"./Parsing/cleanlogs/\"+log_type+\"_labelled.csv\",sep=\",\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_log(data_file = \"./Parsing/cleanlogs/HDFS_structured_clean.csv\",label_file = \"./Parsing/rawlogs/anomaly_label.csv\", log_type = \"HDFS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.vegalite.v4+json": {
       "$schema": "https://vega.github.io/schema/vega-lite/v4.8.1.json",
       "config": {
        "axis": {
         "labelPadding": 5,
         "titleFontSize": 12,
         "titlePadding": 10
        },
        "title": {
         "align": "left",
         "fontSize": 20
        },
        "view": {
         "continuousHeight": 300,
         "continuousWidth": 400
        }
       },
       "data": {
        "url": "http://localhost:21795/cbb6c3250f648abba6e1c7fcf0f6ef10.json"
       },
       "height": 300,
       "layer": [
        {
         "encoding": {
          "fill": {
           "field": "label",
           "legend": null,
           "type": "nominal"
          },
          "x": {
           "axis": {
            "labelAngle": 0,
            "title": null
           },
           "field": "label",
           "sort": "-y",
           "type": "nominal"
          },
          "y": {
           "aggregate": "count",
           "axis": {
            "title": "Number of logs"
           },
           "type": "quantitative"
          }
         },
         "mark": "bar"
        },
        {
         "encoding": {
          "fill": {
           "field": "label",
           "legend": null,
           "type": "nominal"
          },
          "text": {
           "aggregate": "count",
           "type": "quantitative"
          },
          "x": {
           "axis": {
            "labelAngle": 0,
            "title": null
           },
           "field": "label",
           "sort": "-y",
           "type": "nominal"
          },
          "y": {
           "aggregate": "count",
           "axis": {
            "title": "Number of logs"
           },
           "type": "quantitative"
          }
         },
         "mark": {
          "baseline": "bottom",
          "dx": 3,
          "type": "text"
         }
        }
       ],
       "title": "Distribution of labels in HDFS log dataset. ",
       "width": 300
      },
      "image/png": "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",
      "text/plain": [
       "<VegaLite 4 object>\n",
       "\n",
       "If you see this message, it means the renderer has not been properly enabled\n",
       "for the frontend that you are using. For more information, see\n",
       "https://altair-viz.github.io/user_guide/troubleshooting.html\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chart of Anomaly Distribution\n",
    "HDFS_clean = pd.read_csv(\"./Parsing/cleanlogs/HDFS_labelled.csv\")\n",
    "alt.renderers.enable('mimetype')\n",
    "alt.data_transformers.enable('data_server')\n",
    "bars = alt.Chart(HDFS_clean).mark_bar().encode(\n",
    "    alt.X(\"label\",sort=\"-y\", axis=alt.Axis(title= None,labelAngle=0)),\n",
    "    alt.Y(\"count()\",axis=alt.Axis(title=\"Number of logs\")),\n",
    "    alt.Fill(\"label\",legend=None)\n",
    ")\n",
    "\n",
    "text = bars.mark_text(\n",
    "    baseline='bottom',\n",
    "    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n",
    ").encode(\n",
    "    text='count()'\n",
    ")\n",
    "\n",
    "(bars+text).properties(width=300,height=300,title = \"Distribution of labels in HDFS log dataset. \").configure_title(fontSize=20, align=\"left\").configure_axis(labelPadding=5, titleFontSize=12, titlePadding = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from sklearn.utils import shuffle\n",
    "from collections import OrderedDict\n",
    "\n",
    "#2.1 Load by Session Window\n",
    "def _session_window(data_file,labels=False):\n",
    "    ###\n",
    "    ###Only available to HDFS data\n",
    "    ###Seperate data by Session Window (i.e. unique identifier \"blk_id\")\n",
    "    ###\n",
    "    ###Parameters: data_file, str (file path to cleaned datafile)\n",
    "    ###Parameters: labels, boolean (if you have label of logs or not)\n",
    "    ###\n",
    "    ###Output: x_data = array of list, each row of array is a blk_id with list being the eventsequence\n",
    "    ###Output: y_data = array of list, each row of array is blk_id with 0 if Normal or 1 if Anomaly\n",
    "    ###Output: event_df = pd.df of x_data with blk_id and y_label\n",
    "    \n",
    "    #Read in Structured Log (with Labels)\n",
    "    if labels: \n",
    "        struct_log = pd.read_csv(data_file,converters={'blk_id': literal_eval})\n",
    "        int_struct_log=struct_log[[\"EventId\",\"blk_id\",\"label\"]] #Subset struct_log with only columns of interest EventId, blk_id and label\n",
    "        data = int_struct_log.explode(\"blk_id\") #Each blk_id is on it's own row\n",
    "    else:\n",
    "        struct_log = pd.read_csv(data_file,converters={'blk_id': literal_eval})\n",
    "        int_struct_log=struct_log[[\"EventId\",\"blk_id\"]] #Subset struct_log with only columns of interest EventId, blk_id and label\n",
    "        data = int_struct_log.explode(\"blk_id\") #Each blk_id is on it's own row\n",
    "    \n",
    "    #Group Events by blk_id\n",
    "    event_dict = OrderedDict() #Create empty dictionary\n",
    "    for idx,row in data.iterrows(): #For each row in int_struct_log \n",
    "        if not row[\"blk_id\"] in event_dict: #Check if blk_id exists already\n",
    "            event_dict[row[\"blk_id\"]] = [] \n",
    "        event_dict[row[\"blk_id\"]].append(row['EventId']) #Add corresponding EventId to blk_id\n",
    "    event_df = pd.DataFrame(list(event_dict.items()), columns=['BlockId', 'EventSequence']) #Convert data_dict into data_frame\n",
    "    x_data = event_df['EventSequence'].values\n",
    "    \n",
    "    #Determine if blk_id is an Anomaly (1) or Normal (0)\n",
    "    if labels:\n",
    "        label_data = data.set_index('blk_id')\n",
    "        label_dict = label_data['label'].to_dict()\n",
    "        event_df['Label'] = event_df['BlockId'].apply(lambda x: 1 if label_dict[x] == 'Anomaly' else 0)\n",
    "        y_data = event_df[\"Label\"].values\n",
    "    else:\n",
    "        y_data = None\n",
    "    print(\"Number of unique blk_ids: \", len(x_data))    \n",
    "    return (x_data, y_data, event_df)\n",
    "\n",
    "\n",
    "#2.2 Load by Fixed Window\n",
    "def _fixed_window(data_file,windowsize=0,log_type=\"HDFS\"):\n",
    "    ###\n",
    "    ###Available to both HDFS and Zookeeper\n",
    "    ###Seperate un-labelled data by Fixed Window\n",
    "    ###\n",
    "    ###Parameters: data_file, str (file path to cleaned datafile)\n",
    "    ###Parameters: windowsize, int (fixed window size in hours)\n",
    "    ###Parameters: log_type, str (HDFS or Zookeeper)\n",
    "    ###\n",
    "    ###Output: x_data = array of list, each row of array is a blk_id with list being the eventsequence\n",
    "    ###Output: y_data = None\n",
    "    ###Output: event_df = pd.df of x_data with window label\n",
    "    \n",
    "    window_size = windowsize #In Hours\n",
    "    window_start = []\n",
    "    window_end = []\n",
    "    #For Formatting DateTime\n",
    "    iterator = 0 \n",
    "\n",
    "    #Labelling \n",
    "    fix_window_label = [] #Container\n",
    "    count = 1  #Number label for Windows \n",
    "\n",
    "    #Reading in Data\n",
    "    struct_log = pd.read_csv(data_file)\n",
    "    data=struct_log[[\"EventId\",\"Date\",\"Time\"]] #Subset with EventId, Date and Time\n",
    "    \n",
    "    #Create DateTime Column for HDFS File:\n",
    "    if log_type == \"HDFS\":\n",
    "        data[\"Time\"] = data[\"Time\"].astype(str)\n",
    "        #Formatting DateTime\n",
    "        for time in data[\"Time\"]:\n",
    "            if len(time) < 6: #Not in HH:MM:SS format\n",
    "                data.Time[iterator] = (6 - len(time))*\"0\" + time #Fill in empty values with 0\n",
    "            iterator += 1\n",
    "        data[\"DateTime\"] =  \"0\"+data[\"Date\"].astype(str)+data[\"Time\"]\n",
    "        data[\"DateTime\"]= pd.to_datetime(data[\"DateTime\"],format='%y%m%d%H%M%S')\n",
    "    #Create DateTime Column for Zookeeper File\"\n",
    "    elif log_type == \"Zookeeper\":\n",
    "        holder = []\n",
    "        for i in range(0,len(data)):       \n",
    "            holder.append(pd.to_datetime(data[\"Date\"][i] +\" \"+ data[\"Time\"][i].split(\",\")[0],format = '%Y/%m/%d %H:%M:%S'))\n",
    "        data[\"DateTime\"] = holder\n",
    "\n",
    "    #Initializing Time\n",
    "    data = data.sort_values(\"DateTime\") #Sort the DateTime so labelling is sequential \n",
    "    start_time = data[\"DateTime\"][0]\n",
    "    end_time = start_time+pd.DateOffset(hours=window_size)\n",
    "\n",
    "    #Create a list of labels\n",
    "    for time in data[\"DateTime\"]:\n",
    "        if time >= start_time and time <= end_time: #We are within a window\n",
    "            fix_window_label.append(\"window\"+str(count))\n",
    "        elif time > end_time:\n",
    "            while time > end_time: #We are in the next window: \n",
    "                start_time = start_time+pd.DateOffset(hours=window_size)\n",
    "                end_time = start_time+pd.DateOffset(hours=window_size)\n",
    "            count += 1\n",
    "            fix_window_label.append(\"window\"+str(count))\n",
    "        window_start.append(start_time)\n",
    "        window_end.append(end_time)\n",
    "\n",
    "    #Add window_id to dataframe\n",
    "    data[\"window_id\"] = fix_window_label\n",
    "    data[\"window_start\"] = window_start\n",
    "    data[\"window_end\"] = window_end\n",
    "\n",
    "    #Group Events by window_id\n",
    "    event_dict = OrderedDict() #Create empty dictionary\n",
    "    for idx,row in data.iterrows(): #For each row in int_struct_log \n",
    "        if not row[\"window_id\"] in event_dict: #Check if blk_id exists already\n",
    "            event_dict[row[\"window_id\"]] = [] \n",
    "        event_dict[row[\"window_id\"]].append(row['EventId']) #Add corresponding EventId to blk_id\n",
    "    event_df = pd.DataFrame(list(event_dict.items()), columns=['window_id', 'EventSequence']) #Convert data_dict into data_frame\n",
    "    x_data = event_df['EventSequence'].values\n",
    "    y_data = None \n",
    "    \n",
    "    print(\"Number of windows: \",len(x_data))\n",
    "    print(\"Number of window slides: \", count-1)\n",
    "    \n",
    "    return (x_data, y_data, event_df)\n",
    "    \n",
    "#2.3 Load by Sliding window\n",
    "def _sliding_window(data_file,windowsize=0,windowslide=0,log_type=\"HDFS\"):\n",
    "    ###\n",
    "    ###Available to both HDFS and Zookeeper\n",
    "    ###Seperate un-labelled data by Sliding Window\n",
    "    ###\n",
    "    ###Parameters: data_file, str (file path to cleaned datafile)\n",
    "    ###Parameters: windowsize, int (fixed window size in hours)\n",
    "    ###Parameters: windowslide, int (how many hours to sliiiiiideeeeee)\n",
    "    ###Parameters: log_type, str (HDFS or Zookeeper)\n",
    "    ###\n",
    "    ###Output: x_data = array of list, each row of array is a blk_id with list being the eventsequence\n",
    "    ###Output: y_data = None\n",
    "    ###Output: event_df = pd.df of x_data with window label\n",
    "    \n",
    "    #Sliding Window Parameters\n",
    "    window_size = windowsize #In Hours\n",
    "    window_slide = windowslide #In Hours \n",
    "\n",
    "    #For Formatting DateTime\n",
    "    iterator = 0 \n",
    "\n",
    "    #Labelling \n",
    "    count = 1  #Number label for Windows (Ex: window1,window2 ...)\n",
    "\n",
    "    #Reading in Data\n",
    "    struct_log = pd.read_csv(data_file)\n",
    "    data=struct_log[[\"EventId\",\"Date\",\"Time\"]] #Subset with EventId, Date and Time\n",
    "\n",
    "    #Create DateTime Column for HDFS File:\n",
    "    if log_type == \"HDFS\":\n",
    "        data[\"Time\"] = data[\"Time\"].astype(str)\n",
    "        #Formatting DateTime\n",
    "        for time in data[\"Time\"]:\n",
    "            if len(time) < 6: #Not in HH:MM:SS format\n",
    "                data.Time[iterator] = (6 - len(time))*\"0\" + time #Fill in empty values with 0\n",
    "            iterator += 1\n",
    "        data[\"DateTime\"] =  \"0\"+data[\"Date\"].astype(str)+data[\"Time\"]\n",
    "        data[\"DateTime\"]= pd.to_datetime(data[\"DateTime\"],format='%y%m%d%H%M%S')\n",
    "    #Create DateTime Column for Zookeeper File\"\n",
    "    elif log_type == \"Zookeeper\":\n",
    "        holder = []\n",
    "        for i in range(0,len(data)):       \n",
    "            holder.append(pd.to_datetime(data[\"Date\"][i] +\" \"+ data[\"Time\"][i].split(\",\")[0],format = '%Y/%m/%d %H:%M:%S'))\n",
    "        data[\"DateTime\"] = holder\n",
    "\n",
    "    #Initializing Time\n",
    "    data = data.sort_values(\"DateTime\") #Sort the DateTime so labelling is sequential \n",
    "    start_time = data[\"DateTime\"][0] #Earlierst Date \n",
    "    end_time = start_time+pd.DateOffset(hours=window_size) #End of Window\n",
    "    stop_slide = []\n",
    "    event_dict = OrderedDict() #Create empty dictionary\n",
    "    test = 0 \n",
    "    sd = []\n",
    "    #Sliding Window Workframe\n",
    "    while (start_time not in stop_slide): #while (not at end of df) or (not repeating)\n",
    "        slide_window_label = []\n",
    "        #Fixed Window Workframe \n",
    "        #Create a list of labels\n",
    "        for time in data[\"DateTime\"]:\n",
    "            if time >= start_time and time <= end_time: #We are within a window\n",
    "                slide_window_label.append(\"window\"+str(count))\n",
    "            elif time > end_time:\n",
    "                while time > end_time: #In-case there are large time gaps in logs\n",
    "                    start_time = start_time+pd.DateOffset(hours=window_size)\n",
    "                    end_time = start_time+pd.DateOffset(hours=window_size)\n",
    "                count += 1\n",
    "                slide_window_label.append(\"window\"+str(count))\n",
    "                sd.append(start_time)\n",
    "            else: #we are below start time NO RETROGRADING\n",
    "                slide_window_label.append(\"Retrograde\")\n",
    "            stop_slide.append(start_time)\n",
    "        #End of Fixed Window Workframe\n",
    "\n",
    "        #Add window_id to dataframe\n",
    "        data[\"window_id\"] = slide_window_label\n",
    "        #Group Events by window_id\n",
    "        for idx,row in data.iterrows(): #For each row in int_struct_log \n",
    "            if not row[\"window_id\"] in event_dict: #Check if id exists already\n",
    "                event_dict[row[\"window_id\"]] = [] \n",
    "            event_dict[row[\"window_id\"]].append(row['EventId']) #Add corresponding EventId to blk_id\n",
    "\n",
    "        #Reset\n",
    "        start_time = data[\"DateTime\"][0]+pd.DateOffset(hours=window_slide) #Earlierst Date \n",
    "        end_time = start_time+pd.DateOffset(hours=window_size) #End of Window\n",
    "        test +=1\n",
    "    #End of Sliding Workframe\n",
    "    event_df = pd.DataFrame(list(event_dict.items()), columns=['window_id', 'EventSequence']) #Convert data_dict into data_frame\n",
    "    event_df = event_df[event_df.window_id  != \"Retrograde\"] \n",
    "    x_data = event_df['EventSequence'].values\n",
    "    y_data = None \n",
    "\n",
    "    print(\"Number of windows: \",len(x_data))\n",
    "    print(\"Number of window slides: \", count-1)\n",
    "    \n",
    "    return (x_data, y_data, event_df)\n",
    "    \n",
    "#3 Split up: Training:Validation:Testing\n",
    "def _split_data(x_data, y_data=None, train_ratio=0, split_type='uniform'):\n",
    "    ###\n",
    "    ###Split data into Training and Testing\n",
    "    ###\n",
    "    ###Parameters: x_data, list of array (containing event sequence for each window)\n",
    "    ###Parameters: y_data, list (0 for Normal or 1 or Anomaly)\n",
    "    ###Parameters: train_ratio, float (ratio of training to testing data)\n",
    "    ###Parameters: split_type, string ('uniform' for Labelled Data and 'sequential' for non-labelled Data)\n",
    "    ###\n",
    "    ###Output: x_data = array of list, each row of array is a blk_id/window with list being the eventsequence\n",
    "    ###Output: y_data = array of list, each row of array is blk_id/window with 0 if Normal or 1 if Anomaly\n",
    "    ###Output: x_test = array of list, each row of array is a blk_id/window with list being the eventsequence\n",
    "    ###Output: y_test = array of list, each row of array is blk_id/window with 0 if Normal or 1 if Anomaly\n",
    "    \n",
    "    #Uniform when you have lables: Will split Anomalies and Normal equally among test and train set\n",
    "    if split_type == 'uniform' and y_data is not None:\n",
    "        pos_idx = y_data > 0\n",
    "        x_pos = x_data[pos_idx]\n",
    "        y_pos = y_data[pos_idx]\n",
    "        x_neg = x_data[~pos_idx]\n",
    "        y_neg = y_data[~pos_idx]\n",
    "        train_pos = int(train_ratio * x_pos.shape[0])\n",
    "        train_neg = int(train_ratio * x_neg.shape[0])\n",
    "        x_train = np.hstack([x_pos[0:train_pos], x_neg[0:train_neg]])\n",
    "        y_train = np.hstack([y_pos[0:train_pos], y_neg[0:train_neg]])\n",
    "        x_test = np.hstack([x_pos[train_pos:], x_neg[train_neg:]])\n",
    "        y_test = np.hstack([y_pos[train_pos:], y_neg[train_neg:]])\n",
    "    \n",
    "    #Sequential when you DO NOT have lables\n",
    "    elif split_type == 'sequential':\n",
    "        num_train = int(train_ratio * x_data.shape[0])\n",
    "        x_train = x_data[0:num_train]\n",
    "        x_test = x_data[num_train:]\n",
    "        if y_data is None:\n",
    "            y_train = None\n",
    "            y_test = None\n",
    "        else:\n",
    "            y_train = y_data[0:num_train]\n",
    "            y_test = y_data[num_train:]\n",
    "            \n",
    "    # Random shuffle\n",
    "    indexes = shuffle(np.arange(x_train.shape[0]))\n",
    "    x_train = x_train[indexes]\n",
    "    if y_train is not None:\n",
    "        y_train = y_train[indexes]\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of windows:  19\n",
      "Number of window slides:  18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_id</th>\n",
       "      <th>EventSequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>window1</td>\n",
       "      <td>[E2, E2, E13, E2, E2, E13, E13, E13, E2, E3, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>window2</td>\n",
       "      <td>[E11, E11, E11, E11, E11, E11, E3, E13, E10, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>window3</td>\n",
       "      <td>[E13, E3, E10, E3, E3, E6, E2, E6, E2, E2, E13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>window4</td>\n",
       "      <td>[E8, E11, E11, E8, E11, E8, E8, E8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>window5</td>\n",
       "      <td>[E8, E8, E8, E8, E8, E8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>window6</td>\n",
       "      <td>[E8, E11, E8, E8, E8, E8, E8, E8, E11, E8, E11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>window7</td>\n",
       "      <td>[E7, E7, E11, E11, E11, E8, E8, E11, E8, E8, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>window8</td>\n",
       "      <td>[E1, E1, E1, E1, E1, E1, E1, E1, E3, E6, E1, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>window9</td>\n",
       "      <td>[E6, E10, E2, E8, E11, E11, E11, E11, E11, E11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>window10</td>\n",
       "      <td>[E10, E2, E10, E10, E3, E10, E13, E2, E13, E3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>window11</td>\n",
       "      <td>[E8, E11, E11, E8, E7, E11, E8, E11, E8, E11, E7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>window12</td>\n",
       "      <td>[E7, E11, E11, E11, E8, E11, E8, E11, E8, E8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>window13</td>\n",
       "      <td>[E4, E4, E4, E4, E4, E4, E4, E4, E4, E4, E4, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>window14</td>\n",
       "      <td>[E3, E3, E10, E10, E13, E10, E2, E13, E10, E13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>window15</td>\n",
       "      <td>[E11, E8, E11, E11, E8, E8, E8, E11, E8, E8, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>window16</td>\n",
       "      <td>[E3, E8, E3, E13, E13, E13, E10, E3, E10, E6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>window17</td>\n",
       "      <td>[E3, E10, E3, E2, E13, E4, E4, E4, E4, E4, E4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>window18</td>\n",
       "      <td>[E13, E10, E13, E13, E2, E6, E2, E3, E6, E13, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>window19</td>\n",
       "      <td>[E10, E2, E10, E2, E13, E3, E2, E2, E13, E10, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   window_id                                      EventSequence\n",
       "0    window1  [E2, E2, E13, E2, E2, E13, E13, E13, E2, E3, E...\n",
       "1    window2  [E11, E11, E11, E11, E11, E11, E3, E13, E10, E...\n",
       "2    window3  [E13, E3, E10, E3, E3, E6, E2, E6, E2, E2, E13...\n",
       "3    window4                [E8, E11, E11, E8, E11, E8, E8, E8]\n",
       "4    window5                           [E8, E8, E8, E8, E8, E8]\n",
       "5    window6  [E8, E11, E8, E8, E8, E8, E8, E8, E11, E8, E11...\n",
       "6    window7  [E7, E7, E11, E11, E11, E8, E8, E11, E8, E8, E...\n",
       "7    window8  [E1, E1, E1, E1, E1, E1, E1, E1, E3, E6, E1, E...\n",
       "8    window9  [E6, E10, E2, E8, E11, E11, E11, E11, E11, E11...\n",
       "9   window10  [E10, E2, E10, E10, E3, E10, E13, E2, E13, E3,...\n",
       "10  window11  [E8, E11, E11, E8, E7, E11, E8, E11, E8, E11, E7]\n",
       "11  window12      [E7, E11, E11, E11, E8, E11, E8, E11, E8, E8]\n",
       "12  window13  [E4, E4, E4, E4, E4, E4, E4, E4, E4, E4, E4, E...\n",
       "13  window14  [E3, E3, E10, E10, E13, E10, E2, E13, E10, E13...\n",
       "14  window15  [E11, E8, E11, E11, E8, E8, E8, E11, E8, E8, E...\n",
       "15  window16  [E3, E8, E3, E13, E13, E13, E10, E3, E10, E6, ...\n",
       "16  window17  [E3, E10, E3, E2, E13, E4, E4, E4, E4, E4, E4,...\n",
       "17  window18  [E13, E10, E13, E13, E2, E6, E2, E3, E6, E13, ...\n",
       "18  window19  [E10, E2, E10, E2, E13, E3, E2, E2, E13, E10, ..."
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y,df = _sliding_window(data_file=\"./Parsing/cleanlogs/HDFS_structured_clean.csv\",windowsize = 2,windowslide=2, log_type = \"HDFS\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of windows:  19\n",
      "Number of window slides:  18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_id</th>\n",
       "      <th>EventSequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>window1</td>\n",
       "      <td>[E2, E2, E13, E2, E2, E13, E13, E13, E2, E3, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>window2</td>\n",
       "      <td>[E11, E11, E11, E11, E11, E11, E3, E13, E10, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>window3</td>\n",
       "      <td>[E13, E3, E10, E3, E3, E6, E2, E6, E2, E2, E13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>window4</td>\n",
       "      <td>[E8, E11, E11, E8, E11, E8, E8, E8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>window5</td>\n",
       "      <td>[E8, E8, E8, E8, E8, E8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>window6</td>\n",
       "      <td>[E8, E11, E8, E8, E8, E8, E8, E8, E11, E8, E11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>window7</td>\n",
       "      <td>[E7, E7, E11, E11, E11, E8, E8, E11, E8, E8, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>window8</td>\n",
       "      <td>[E1, E1, E1, E1, E1, E1, E1, E1, E3, E6, E1, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>window9</td>\n",
       "      <td>[E6, E10, E2, E8, E11, E11, E11, E11, E11, E11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>window10</td>\n",
       "      <td>[E10, E2, E10, E10, E3, E10, E13, E2, E13, E3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>window11</td>\n",
       "      <td>[E8, E11, E11, E8, E7, E11, E8, E11, E8, E11, E7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>window12</td>\n",
       "      <td>[E7, E11, E11, E11, E8, E11, E8, E11, E8, E8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>window13</td>\n",
       "      <td>[E4, E4, E4, E4, E4, E4, E4, E4, E4, E4, E4, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>window14</td>\n",
       "      <td>[E3, E3, E10, E10, E13, E10, E2, E13, E10, E13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>window15</td>\n",
       "      <td>[E11, E8, E11, E11, E8, E8, E8, E11, E8, E8, E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>window16</td>\n",
       "      <td>[E3, E8, E3, E13, E13, E13, E10, E3, E10, E6, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>window17</td>\n",
       "      <td>[E3, E10, E3, E2, E13, E4, E4, E4, E4, E4, E4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>window18</td>\n",
       "      <td>[E13, E10, E13, E13, E2, E6, E2, E3, E6, E13, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>window19</td>\n",
       "      <td>[E10, E2, E10, E2, E13, E3, E2, E2, E13, E10, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   window_id                                      EventSequence\n",
       "0    window1  [E2, E2, E13, E2, E2, E13, E13, E13, E2, E3, E...\n",
       "1    window2  [E11, E11, E11, E11, E11, E11, E3, E13, E10, E...\n",
       "2    window3  [E13, E3, E10, E3, E3, E6, E2, E6, E2, E2, E13...\n",
       "3    window4                [E8, E11, E11, E8, E11, E8, E8, E8]\n",
       "4    window5                           [E8, E8, E8, E8, E8, E8]\n",
       "5    window6  [E8, E11, E8, E8, E8, E8, E8, E8, E11, E8, E11...\n",
       "6    window7  [E7, E7, E11, E11, E11, E8, E8, E11, E8, E8, E...\n",
       "7    window8  [E1, E1, E1, E1, E1, E1, E1, E1, E3, E6, E1, E...\n",
       "8    window9  [E6, E10, E2, E8, E11, E11, E11, E11, E11, E11...\n",
       "9   window10  [E10, E2, E10, E10, E3, E10, E13, E2, E13, E3,...\n",
       "10  window11  [E8, E11, E11, E8, E7, E11, E8, E11, E8, E11, E7]\n",
       "11  window12      [E7, E11, E11, E11, E8, E11, E8, E11, E8, E8]\n",
       "12  window13  [E4, E4, E4, E4, E4, E4, E4, E4, E4, E4, E4, E...\n",
       "13  window14  [E3, E3, E10, E10, E13, E10, E2, E13, E10, E13...\n",
       "14  window15  [E11, E8, E11, E11, E8, E8, E8, E11, E8, E8, E...\n",
       "15  window16  [E3, E8, E3, E13, E13, E13, E10, E3, E10, E6, ...\n",
       "16  window17  [E3, E10, E3, E2, E13, E4, E4, E4, E4, E4, E4,...\n",
       "17  window18  [E13, E10, E13, E13, E2, E6, E2, E3, E6, E13, ...\n",
       "18  window19  [E10, E2, E10, E2, E13, E3, E2, E2, E13, E10, ..."
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y,df = _fixed_window(data_file=\"./Parsing/cleanlogs/HDFS_labelled.csv\",windowsize = 2,log_type = \"HDFS\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,df = _fixed_window(data_file=\"./Parsing/cleanlogs/Zookeeper_structured_clean.csv\",windowsize=2,log_type=\"Zookeeper\")\n",
    "x,y,df = _session_window(data_file=\"./Parsing/cleanlogs/HDFS_labelled.csv\",labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03400222965440357\n",
      "0.035\n"
     ]
    }
   ],
   "source": [
    "xdata, ydata = _session_window(\"./Parsing/cleanlogs/HDFS_labelled.csv\",labels=True) #Load data if you have labels\n",
    "(xtrain,ytrain),(xtest,ytest) = _split_data(xdata,ydata,train_ratio=0.9,split_type=\"uniform\") #Use Uniform with labels - divides anomalies:normal evenly among test and training set\n",
    "print(sum(ytrain)/len(xtrain))\n",
    "print(sum(ytest)/len(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "x,y,df = _fixed_window(data_file=\"./Parsing/cleanlogs/Zookeeper_structured_clean.csv\",windowsize=2,log_type=\"Zookeeper\")\n",
    "(xtrain,_),(xtest,_) = _split_data(x,y_data=y,train_ratio=0.5,split_type=\"sequential\") #Use sequential when no labels\n",
    "print(len(xtrain))\n",
    "print(len(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997\n",
      "997\n"
     ]
    }
   ],
   "source": [
    "xdata, ydata = _session_window(\"./Parsing/cleanlogs/HDFS_labelled.csv\",labels=False) #Load data if you do not have labels\n",
    "(x_train, _),( x_test, _) = _split_data(xdata,y_data=None,train_ratio=0.5,split_type=\"sequential\") #Use sequential when no labels\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Transformed train data summary ======\n",
      "Train data shape: 997-by-12\n",
      "\n",
      "====== Transformed test data summary ======\n",
      "Test data shape: 997-by-12\n",
      "\n",
      "====== Model summary ======\n",
      "n_components: 9\n",
      "Project matrix shape: 12-by-12\n",
      "SPE threshold: 0.7614555551112776\n",
      "\n",
      "Train validation:\n",
      "====== Evaluation summary ======\n",
      "Precision: 1.000, recall: 0.059, F1-measure: 0.111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Example of Supervised PCA Method \n",
    "\n",
    "from Models.loglizer.loglizer.models import PCA\n",
    "from Models.loglizer.loglizer import preprocessing\n",
    "\n",
    "xdata, ydata = _session_window(\"./Parsing/cleanlogs/HDFS_labelled.csv\",labels=True) #Load data if you have labels\n",
    "(x_train,y_train),(x_test,y_test) = _split_data(xdata,ydata,train_ratio=0.5,split_type=\"uniform\") #Use Uniform with labels - divides anomalies:normal evenly among test and training set\n",
    "\n",
    "feature_extractor = preprocessing.FeatureExtractor()\n",
    "x_train = feature_extractor.fit_transform(x_train, term_weighting='tf-idf', \n",
    "                                          normalization='zero-mean')\n",
    "x_test = feature_extractor.transform(x_test)\n",
    "\n",
    "model = PCA()\n",
    "model.fit(x_train)\n",
    "\n",
    "print('Train validation:')\n",
    "precision, recall, f1 = model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of Supervised PCA Method \n",
    "xdata, ydata = _fixed_window(\"./Parsing/cleanlogs/HDFS_labelled.csv\") #Load data if you have labels\n",
    "(xtrain,ytrain),(xtest,ytest) = _split_data(xdata,ydata,train_ratio=0.5,split_type=\"sequential\") #Use Uniform with labels - divides anomalies:normal evenly among test and training set\n",
    "\n",
    "feature_extractor = preprocessing.FeatureExtractor()\n",
    "x_train = feature_extractor.fit_transform(x_train, term_weighting='tf-idf', \n",
    "                                          normalization='zero-mean')\n",
    "model = PCA()\n",
    "model.fit(x_train)\n",
    "\n",
    "y_train = model.predict(x_train)\n",
    "\n",
    "print('Train validation:')\n",
    "precision, recall, f1 = model.evaluate(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
